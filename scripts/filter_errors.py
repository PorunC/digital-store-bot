#!/usr/bin/env python3
"""
æ—¥å¿—é”™è¯¯è¿‡æ»¤å’Œåˆ†æè„šæœ¬
ç”¨äºä»æ—¥å¿—æ–‡ä»¶ä¸­æå–ã€åˆ†ç±»å’Œç»Ÿè®¡é”™è¯¯ä¿¡æ¯
"""

import re
import argparse
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json


class LogError:
    """è¡¨ç¤ºä¸€ä¸ªæ—¥å¿—é”™è¯¯æ¡ç›®"""
    
    def __init__(self, timestamp: str, level: str, source: str, message: str, line_number: int):
        self.timestamp = timestamp
        self.level = level.upper()
        self.source = source
        self.message = message
        self.line_number = line_number
        self._normalized_message = None
        
    def __str__(self) -> str:
        return f"[{self.timestamp}] {self.level} ({self.source}): {self.message}"
    
    def get_normalized_message(self) -> str:
        """è·å–æ ‡å‡†åŒ–çš„é”™è¯¯æ¶ˆæ¯ï¼ˆå»æ‰æ—¶é—´æˆ³å’ŒåŠ¨æ€å†…å®¹ï¼‰"""
        if self._normalized_message is None:
            self._normalized_message = self._normalize_message(self.message)
        return self._normalized_message
    
    def _normalize_message(self, message: str) -> str:
        """æ ‡å‡†åŒ–é”™è¯¯æ¶ˆæ¯ï¼Œå»æ‰æ—¶é—´æˆ³ã€IDã€æ•°å€¼ç­‰åŠ¨æ€å†…å®¹"""
        normalized = message
        
        # å»æ‰æ—¶é—´æˆ³ (YYYY-MM-DD HH:MM:SS.mmm)
        normalized = re.sub(r'\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}(?:\.\d+)?(?:\s+\w+)?', '[TIMESTAMP]', normalized)
        
        # å»æ‰å„ç§IDæ ¼å¼
        normalized = re.sub(r'\[\d+\]', '[ID]', normalized)  # [123]
        normalized = re.sub(r'\bid:\s*\d+\b', 'id: [ID]', normalized, flags=re.IGNORECASE)  # id: 123
        normalized = re.sub(r'\buuid:\s*[a-f0-9-]+\b', 'uuid: [UUID]', normalized, flags=re.IGNORECASE)
        
        # å»æ‰æ•°å€¼
        normalized = re.sub(r'\b\d+\b', '[NUM]', normalized)
        
        # å»æ‰æ–‡ä»¶è·¯å¾„ä¸­çš„å…·ä½“è·¯å¾„
        normalized = re.sub(r'/[/\w.-]+/', '/[PATH]/', normalized)
        
        # å»æ‰SQLå‚æ•°
        normalized = re.sub(r'\[parameters:\s*\([^)]+\)\]', '[parameters: [...]]', normalized)
        
        # å»æ‰å…·ä½“çš„datetimeå¯¹è±¡
        normalized = re.sub(r'datetime\.datetime\([^)]+\)', 'datetime.datetime([...])', normalized)
        
        # å»æ‰å…·ä½“çš„å¼‚å¸¸åœ°å€
        normalized = re.sub(r'0x[a-f0-9]+', '0x[ADDR]', normalized)
        
        # å»æ‰SQLAlchemyå…·ä½“é”™è¯¯URL
        normalized = re.sub(r'https://sqlalche\.me/e/\w+/\w+', 'https://sqlalche.me/e/[CODE]/[ID]', normalized)
        
        # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
        normalized = re.sub(r'\s+', ' ', normalized)
        normalized = normalized.strip()
        
        return normalized
    
    def get_deduplication_key(self) -> str:
        """è·å–ç”¨äºå»é‡çš„é”®"""
        return f"{self.level}|{self.source}|{self.get_normalized_message()}"
    
    def to_dict(self) -> Dict:
        return {
            'timestamp': self.timestamp,
            'level': self.level,
            'source': self.source,
            'message': self.message,
            'normalized_message': self.get_normalized_message(),
            'line_number': self.line_number
        }


class LogErrorFilter:
    """æ—¥å¿—é”™è¯¯è¿‡æ»¤å™¨"""
    
    def __init__(self, log_file_path: str):
        self.log_file_path = Path(log_file_path)
        self.errors: List[LogError] = []
        
        # åŒ¹é…ä¸åŒæ ¼å¼çš„é”™è¯¯æ—¥å¿—
        self.error_patterns = [
            # PostgreSQL é”™è¯¯æ ¼å¼: postgres | 2025-07-23 09:06:29.346 UTC [59] ERROR: message
            re.compile(r'^(\w+)\s+\|\s+([\d-]+\s+[\d:.]+\s+\w+)\s+\[\d+\]\s+(ERROR):\s+(.+)$'),
            # é€šç”¨é”™è¯¯æ ¼å¼: [timestamp] ERROR source: message
            re.compile(r'^\[([\d-]+\s+[\d:.]+)\]\s+(ERROR|CRITICAL|FATAL)\s+(\w+):\s+(.+)$'),
            # Python å¼‚å¸¸æ ¼å¼
            re.compile(r'^([\d-]+\s+[\d:.]+).*?(ERROR|Exception|Traceback).*?:\s*(.+)$'),
            # ç®€å•é”™è¯¯æ ¼å¼: ERROR: message
            re.compile(r'^(ERROR|CRITICAL|FATAL):\s+(.+)$'),
        ]
    
    def parse_log_file(self) -> None:
        """è§£ææ—¥å¿—æ–‡ä»¶ï¼Œæå–é”™è¯¯ä¿¡æ¯"""
        if not self.log_file_path.exists():
            raise FileNotFoundError(f"æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {self.log_file_path}")
        
        with open(self.log_file_path, 'r', encoding='utf-8', errors='ignore') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«é”™è¯¯å…³é”®è¯
                if not any(keyword in line.upper() for keyword in ['ERROR', 'CRITICAL', 'FATAL', 'EXCEPTION']):
                    continue
                
                error = self._parse_error_line(line, line_num)
                if error:
                    self.errors.append(error)
    
    def _parse_error_line(self, line: str, line_num: int) -> Optional[LogError]:
        """è§£æå•è¡Œé”™è¯¯æ—¥å¿—"""
        for pattern in self.error_patterns:
            match = pattern.match(line)
            if match:
                groups = match.groups()
                
                if len(groups) == 4:  # PostgreSQL æ ¼å¼
                    source, timestamp, level, message = groups
                    return LogError(timestamp, level, source, message, line_num)
                elif len(groups) == 4:  # é€šç”¨æ ¼å¼
                    timestamp, level, source, message = groups
                    return LogError(timestamp, level, source, message, line_num)
                elif len(groups) == 3:  # Pythonå¼‚å¸¸æ ¼å¼
                    timestamp, level, message = groups
                    return LogError(timestamp, level, 'python', message, line_num)
                elif len(groups) == 2:  # ç®€å•æ ¼å¼
                    level, message = groups
                    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    return LogError(timestamp, level, 'unknown', message, line_num)
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°æ¨¡å¼ï¼Œä½†åŒ…å«é”™è¯¯å…³é”®è¯ï¼Œåˆ›å»ºé€šç”¨é”™è¯¯
        if any(keyword in line.upper() for keyword in ['ERROR', 'CRITICAL', 'FATAL']):
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            return LogError(timestamp, 'ERROR', 'unknown', line, line_num)
        
        return None
    
    def get_error_statistics(self) -> Dict:
        """è·å–é”™è¯¯ç»Ÿè®¡ä¿¡æ¯"""
        if not self.errors:
            return {}
        
        # æŒ‰é”™è¯¯çº§åˆ«ç»Ÿè®¡
        level_counts = Counter(error.level for error in self.errors)
        
        # æŒ‰æ¥æºç»Ÿè®¡
        source_counts = Counter(error.source for error in self.errors)
        
        # æŒ‰é”™è¯¯ç±»å‹ç»Ÿè®¡ï¼ˆåŸºäºæ¶ˆæ¯å…³é”®è¯ï¼‰
        error_types = defaultdict(int)
        for error in self.errors:
            # æå–é”™è¯¯ç±»å‹å…³é”®è¯
            message_lower = error.message.lower()
            if 'foreign key' in message_lower:
                error_types['å¤–é”®çº¦æŸé”™è¯¯'] += 1
            elif 'connection' in message_lower:
                error_types['è¿æ¥é”™è¯¯'] += 1
            elif 'permission' in message_lower or 'denied' in message_lower:
                error_types['æƒé™é”™è¯¯'] += 1
            elif 'timeout' in message_lower:
                error_types['è¶…æ—¶é”™è¯¯'] += 1
            elif 'not found' in message_lower:
                error_types['æœªæ‰¾åˆ°é”™è¯¯'] += 1
            else:
                error_types['å…¶ä»–é”™è¯¯'] += 1
        
        # æ—¶é—´åˆ†å¸ƒç»Ÿè®¡
        time_distribution = defaultdict(int)
        for error in self.errors:
            try:
                # æå–å°æ—¶ä¿¡æ¯è¿›è¡Œç»Ÿè®¡
                if ':' in error.timestamp:
                    hour = error.timestamp.split()[1].split(':')[0]
                    time_distribution[f"{hour}:00"] += 1
            except (IndexError, ValueError):
                continue
        
        return {
            'total_errors': len(self.errors),
            'level_distribution': dict(level_counts),
            'source_distribution': dict(source_counts),
            'error_types': dict(error_types),
            'time_distribution': dict(sorted(time_distribution.items())),
            'top_errors': self._get_top_error_messages(10)
        }
    
    def _get_top_error_messages(self, limit: int = 10) -> List[Tuple[str, int]]:
        """è·å–å‡ºç°é¢‘ç‡æœ€é«˜çš„é”™è¯¯æ¶ˆæ¯"""
        message_counts = Counter()
        
        for error in self.errors:
            # ä½¿ç”¨æ ‡å‡†åŒ–çš„é”™è¯¯æ¶ˆæ¯è¿›è¡Œç»Ÿè®¡
            normalized_message = error.get_normalized_message()
            message_counts[normalized_message] += 1
        
        return message_counts.most_common(limit)
    
    def deduplicate_errors(self, errors: Optional[List[LogError]] = None) -> List[LogError]:
        """å¯¹é”™è¯¯è¿›è¡Œå»é‡ï¼Œä¿ç•™æ¯ç§é”™è¯¯çš„ç¬¬ä¸€æ¬¡å‡ºç°"""
        errors_to_process = errors or self.errors
        
        seen_keys = set()
        deduplicated_errors = []
        duplicate_counts = Counter()
        
        for error in errors_to_process:
            dedup_key = error.get_deduplication_key()
            duplicate_counts[dedup_key] += 1
            
            if dedup_key not in seen_keys:
                seen_keys.add(dedup_key)
                deduplicated_errors.append(error)
        
        # ä¸ºå»é‡åçš„é”™è¯¯æ·»åŠ è®¡æ•°ä¿¡æ¯
        for error in deduplicated_errors:
            dedup_key = error.get_deduplication_key()
            error.occurrence_count = duplicate_counts[dedup_key]
        
        return deduplicated_errors
    
    def get_deduplication_statistics(self, errors: Optional[List[LogError]] = None) -> Dict:
        """è·å–å»é‡ç»Ÿè®¡ä¿¡æ¯"""
        errors_to_process = errors or self.errors
        deduplicated = self.deduplicate_errors(errors_to_process)
        
        # ç»Ÿè®¡é‡å¤æ¬¡æ•°åˆ†å¸ƒ
        occurrence_distribution = Counter()
        for error in deduplicated:
            count = getattr(error, 'occurrence_count', 1)
            occurrence_distribution[count] += 1
        
        # æ‰¾å‡ºé‡å¤æ¬¡æ•°æœ€å¤šçš„é”™è¯¯
        top_duplicates = []
        for error in deduplicated:
            count = getattr(error, 'occurrence_count', 1)
            if count > 1:
                top_duplicates.append((error.get_normalized_message(), count))
        
        top_duplicates.sort(key=lambda x: x[1], reverse=True)
        
        return {
            'original_count': len(errors_to_process),
            'deduplicated_count': len(deduplicated),
            'reduction_percentage': round((1 - len(deduplicated) / len(errors_to_process)) * 100, 2) if errors_to_process else 0,
            'occurrence_distribution': dict(occurrence_distribution),
            'top_duplicates': top_duplicates[:10]
        }
    
    def filter_errors(self, 
                     level: Optional[str] = None,
                     source: Optional[str] = None,
                     keyword: Optional[str] = None,
                     start_time: Optional[str] = None,
                     end_time: Optional[str] = None) -> List[LogError]:
        """æ ¹æ®æ¡ä»¶è¿‡æ»¤é”™è¯¯"""
        filtered_errors = self.errors
        
        if level:
            filtered_errors = [e for e in filtered_errors if e.level.upper() == level.upper()]
        
        if source:
            filtered_errors = [e for e in filtered_errors if source.lower() in e.source.lower()]
        
        if keyword:
            filtered_errors = [e for e in filtered_errors if keyword.lower() in e.message.lower()]
        
        # æ—¶é—´è¿‡æ»¤ï¼ˆç®€å•å®ç°ï¼‰
        if start_time or end_time:
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„æ—¶é—´è¿‡æ»¤é€»è¾‘
            pass
        
        return filtered_errors
    
    def export_to_json(self, output_file: str, filtered_errors: Optional[List[LogError]] = None) -> None:
        """å¯¼å‡ºé”™è¯¯åˆ°JSONæ–‡ä»¶"""
        errors_to_export = filtered_errors or self.errors
        data = {
            'export_time': datetime.now().isoformat(),
            'total_errors': len(errors_to_export),
            'errors': [error.to_dict() for error in errors_to_export]
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"é”™è¯¯æ•°æ®å·²å¯¼å‡ºåˆ°: {output_file}")
    
    def print_summary(self, filtered_errors: Optional[List[LogError]] = None, show_deduplication: bool = False) -> None:
        """æ‰“å°é”™è¯¯æ‘˜è¦"""
        errors_to_analyze = filtered_errors or self.errors
        
        if not errors_to_analyze:
            print("æ²¡æœ‰æ‰¾åˆ°é”™è¯¯ä¿¡æ¯ã€‚")
            return
        
        stats = self.get_error_statistics()
        
        print(f"\n{'='*60}")
        print(f"æ—¥å¿—é”™è¯¯åˆ†ææŠ¥å‘Š")
        print(f"{'='*60}")
        print(f"æ—¥å¿—æ–‡ä»¶: {self.log_file_path}")
        print(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æ€»é”™è¯¯æ•°: {stats['total_errors']}")
        
        # æ˜¾ç¤ºå»é‡ä¿¡æ¯
        if show_deduplication:
            dedup_stats = self.get_deduplication_statistics(errors_to_analyze)
            print(f"\nğŸ”„ å»é‡ç»Ÿè®¡:")
            print(f"  åŸå§‹é”™è¯¯æ•°: {dedup_stats['original_count']}")
            print(f"  å»é‡åé”™è¯¯æ•°: {dedup_stats['deduplicated_count']}")
            print(f"  å‡å°‘ç™¾åˆ†æ¯”: {dedup_stats['reduction_percentage']}%")
            
            if dedup_stats['top_duplicates']:
                print(f"\nğŸ” é«˜é¢‘é‡å¤é”™è¯¯ (å‰5):")
                for i, (message, count) in enumerate(dedup_stats['top_duplicates'][:5], 1):
                    print(f"  {i}. [{count}æ¬¡] {message[:80]}...")
        
        print(f"\nğŸ“Š é”™è¯¯çº§åˆ«åˆ†å¸ƒ:")
        for level, count in stats['level_distribution'].items():
            print(f"  {level}: {count}")
        
        print(f"\nğŸ” é”™è¯¯æ¥æºåˆ†å¸ƒ:")
        for source, count in stats['source_distribution'].items():
            print(f"  {source}: {count}")
        
        print(f"\nğŸ“ é”™è¯¯ç±»å‹åˆ†å¸ƒ:")
        for error_type, count in stats['error_types'].items():
            print(f"  {error_type}: {count}")
        
        if stats['time_distribution']:
            print(f"\nâ° æ—¶é—´åˆ†å¸ƒ:")
            for time_slot, count in list(stats['time_distribution'].items())[:10]:
                print(f"  {time_slot}: {count}")
        
        print(f"\nğŸ”¥ é«˜é¢‘é”™è¯¯æ¶ˆæ¯ (å‰10):")
        for i, (message, count) in enumerate(stats['top_errors'], 1):
            print(f"  {i}. [{count}æ¬¡] {message[:80]}...")
        
        print(f"\nğŸ“‹ æœ€è¿‘çš„é”™è¯¯è¯¦æƒ… (å‰5æ¡):")
        for i, error in enumerate(errors_to_analyze[-5:], 1):
            print(f"  {i}. è¡Œ{error.line_number}: {error}")


def main():
    parser = argparse.ArgumentParser(description='æ—¥å¿—é”™è¯¯è¿‡æ»¤å’Œåˆ†æå·¥å…·')
    parser.add_argument('log_file', help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--level', help='è¿‡æ»¤é”™è¯¯çº§åˆ« (ERROR, CRITICAL, FATAL)')
    parser.add_argument('--source', help='è¿‡æ»¤é”™è¯¯æ¥æº')
    parser.add_argument('--keyword', help='è¿‡æ»¤åŒ…å«å…³é”®è¯çš„é”™è¯¯')
    parser.add_argument('--export', help='å¯¼å‡ºç»“æœåˆ°JSONæ–‡ä»¶')
    parser.add_argument('--deduplicate', '-d', action='store_true', help='å¯¹é”™è¯¯è¿›è¡Œå»é‡ï¼ˆå»æ‰æ—¶é—´æˆ³ç­‰åŠ¨æ€å†…å®¹ï¼‰')
    parser.add_argument('--dedup-only', action='store_true', help='åªå¯¼å‡ºå»é‡åçš„é”™è¯¯')
    parser.add_argument('--quiet', '-q', action='store_true', help='é™é»˜æ¨¡å¼ï¼Œåªæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºè¿‡æ»¤å™¨å¹¶è§£ææ—¥å¿—
        filter_obj = LogErrorFilter(args.log_file)
        print(f"æ­£åœ¨è§£ææ—¥å¿—æ–‡ä»¶: {args.log_file}")
        filter_obj.parse_log_file()
        
        # åº”ç”¨è¿‡æ»¤æ¡ä»¶
        filtered_errors = filter_obj.filter_errors(
            level=args.level,
            source=args.source,
            keyword=args.keyword
        )
        
        # åº”ç”¨å»é‡
        if args.deduplicate or args.dedup_only:
            filtered_errors = filter_obj.deduplicate_errors(filtered_errors)
            print(f"å»é‡åé”™è¯¯æ•°: {len(filtered_errors)}")
        
        # å¯¼å‡ºç»“æœ
        if args.export:
            # å¦‚æœä½¿ç”¨äº†å»é‡ï¼Œåœ¨å¯¼å‡ºæ•°æ®ä¸­æ·»åŠ å»é‡ä¿¡æ¯
            if args.deduplicate or args.dedup_only:
                export_data = {
                    'export_time': datetime.now().isoformat(),
                    'original_count': len(filter_obj.errors),
                    'filtered_count': len(filtered_errors),
                    'deduplicated': True,
                    'deduplication_stats': filter_obj.get_deduplication_statistics(filtered_errors),
                    'errors': []
                }
                
                for error in filtered_errors:
                    error_dict = error.to_dict()
                    error_dict['occurrence_count'] = getattr(error, 'occurrence_count', 1)
                    export_data['errors'].append(error_dict)
                
                with open(args.export, 'w', encoding='utf-8') as f:
                    json.dump(export_data, f, ensure_ascii=False, indent=2)
            else:
                filter_obj.export_to_json(args.export, filtered_errors)
            
            print(f"é”™è¯¯æ•°æ®å·²å¯¼å‡ºåˆ°: {args.export}")
        
        # æ˜¾ç¤ºç»“æœ
        if not args.quiet:
            show_dedup = args.deduplicate or args.dedup_only
            filter_obj.print_summary(filtered_errors, show_deduplication=show_dedup)
        else:
            stats = filter_obj.get_error_statistics()
            print(f"æ€»é”™è¯¯æ•°: {stats['total_errors']}")
            print(f"è¿‡æ»¤åé”™è¯¯æ•°: {len(filtered_errors)}")
            
            if args.deduplicate or args.dedup_only:
                dedup_stats = filter_obj.get_deduplication_statistics(filtered_errors)
                print(f"å»é‡å‡å°‘: {dedup_stats['reduction_percentage']}%")
    
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        return 1
    
    return 0


if __name__ == '__main__':
    exit(main())